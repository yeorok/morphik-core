[api]
host = "0.0.0.0"
port = 8000
reload = true     # 개발 모드에서 코드 변경 시 자동 재시작
# reload = false    # 상용 모드: 자동 재시작 비활성화 (안정성 향상)

[auth]
jwt_algorithm = "HS256"
dev_mode = true  # 로컬 개발을 위한 개발 모드 활성화
dev_entity_id = "dev_user"  # 기본 개발 사용자 ID
dev_entity_type = "developer"  # 기본 개발 사용자 타입
dev_permissions = ["read", "write", "admin"]  # 기본 개발 권한

# 상용 환경 설정
#dev_mode = false  # 상용 모드: 보안 강화, 인증 필수
#dev_entity_id = "dev_user"      # 상용에서는 사용하지 않음 (주석 처리)
#dev_entity_type = "developer"   # 상용에서는 사용하지 않음 (주석 처리)
#dev_permissions = ["read", "write", "admin"]  # 상용에서는 사용하지 않음 (주석 처리)

# 상용 모드에서는 JWT 토큰을 통한 적절한 인증이 필요합니다
# JWT_SECRET_KEY 환경변수를 반드시 안전한 값으로 설정하세요


#### 등록된 모델 설정 ####
[registered_models]
# OpenAI models
openai_gpt4-1 = { model_name = "gpt-4.1" }
openai_gpt4-1-mini = { model_name = "gpt-4.1-mini" }

# Azure OpenAI models
azure_gpt4 = { model_name = "gpt-4", api_base = "YOUR_AZURE_URL_HERE", api_version = "2023-05-15", deployment_id = "gpt-4-deployment" }
azure_gpt35 = { model_name = "gpt-3.5-turbo", api_base = "YOUR_AZURE_URL_HERE", api_version = "2023-05-15", deployment_id = "gpt-35-turbo-deployment" }

# Anthropic models
claude_opus = { model_name = "claude-3-opus-20240229" }
claude_sonnet = { model_name = "claude-3-7-sonnet-latest" }

# Ollama models (modify api_base based on your deployment)
# - Local Ollama: "http://localhost:11434" (default)
# - Morphik in Docker, Ollama local: "http://host.docker.internal:11434"
# - Both in Docker: "http://ollama:11434"
ollama_qwen_vision = { model_name = "ollama_chat/qwen2.5vl:latest", api_base = "http://nicemso.ipdisk.co.kr:11434", vision = true }
ollama_llama_vision = { model_name = "ollama_chat/llama3.2-vision", api_base = "http://nicemso.ipdisk.co.kr:11434", vision = true }
ollama_embedding = { model_name = "ollama/nomic-embed-text", api_base = "http://nicemso.ipdisk.co.kr:11434" }

openai_embedding = { model_name = "text-embedding-3-small" }
openai_embedding_large = { model_name = "text-embedding-3-large" }
azure_embedding = { model_name = "text-embedding-ada-002", api_base = "YOUR_AZURE_URL_HERE", api_version = "2023-05-15", deployment_id = "embedding-ada-002" }


#### Component configurations ####

[agent]
# AI 에이전트 로직에 사용할 모델 (외부 Ollama 서버의 비전 모델 사용)
model = "ollama_qwen_vision" # Model for the agent logic

[completion]
# 텍스트 생성/완성에 사용할 모델 (외부 Ollama 서버 사용)
model = "ollama_qwen_vision"  # 또는 "openai_gpt4-1-mini" (OpenAI 사용시)
default_max_tokens = "1000"   # 기본 최대 토큰 수
default_temperature = 0.3     # 창의성 수준 (0.0-1.0, 낮을수록 일관성 높음)

[database]
provider = "postgres"

# 연결 풀 설정 (성능 최적화)
pool_size = 10           # 연결 풀의 최대 연결 수
max_overflow = 15        # 풀 크기 초과시 생성 가능한 추가 연결 수
pool_recycle = 3600      # 연결 재활용 시간 (초, 1시간)
pool_timeout = 10        # 연결 풀에서 연결 대기 시간 (초)
pool_pre_ping = true     # 연결 사용 전 유효성 검사
max_retries = 3          # 데이터베이스 작업 재시도 횟수
retry_delay = 1.0        # 재시도 간 초기 지연 시간 (초)

# 상용 환경 연결 풀 설정 (고성능/고가용성)
#pool_size = 20           # 연결 풀 크기 증가 (높은 동시성 처리)
#max_overflow = 30        # 풀 크기 초과시 생성 가능한 추가 연결 수 증가
#pool_recycle = 7200      # 연결 재활용 시간 (초, 2시간으로 증가)
#pool_timeout = 30        # 연결 풀에서 연결 대기 시간 증가 (초)
#pool_pre_ping = true     # 연결 사용 전 유효성 검사 (필수)
#max_retries = 5          # 데이터베이스 작업 재시도 횟수 증가
#retry_delay = 2.0        # 재시도 간 초기 지연 시간 증가 (초)



[embedding]
# 벡터 임베딩 설정 (문서 검색을 위한 벡터화)
model = "ollama_embedding"    # 외부 Ollama 서버의 임베딩 모델 사용
dimensions = 768              # 벡터 차원 수 (nomic-embed-text 모델의 차원)
similarity_metric = "cosine"  # 유사도 계산 방법 (코사인 유사도)
[parser]
chunk_size = 6000
chunk_overlap = 300
use_unstructured_api = false
use_contextual_chunking = false
contextual_chunking_model = "ollama_qwen_vision"  # Reference to a key in registered_models

[document_analysis]
model = "ollama_qwen_vision"  # Reference to a key in registered_models

[parser.vision]
model = "ollama_qwen_vision"  # Reference to a key in registered_models
frame_sample_rate = -1  # Set to -1 to disable frame captioning

[reranker]
use_reranker = true
provider = "flag"
model_name = "BAAI/bge-reranker-large"
query_max_length = 256
passage_max_length = 512
use_fp16 = true
device = "cpu"  # 외부 Ollama 사용시 CPU만 필요

[storage]
provider = "local"
storage_path = "./storage"

# [storage]
# provider = "aws-s3"
# region = "us-east-2"
# bucket_name = "morphik-s3-storage"

[vector_store]
provider = "pgvector"

[rules]
model = "ollama_qwen_vision"
batch_size = 4096

[morphik]
enable_colpali = true
mode = "self_hosted"  # "cloud" or "self_hosted"
#api_domain = "api.morphik.ai"  # API domain for cloud URIs
api_domain = "morphik-api.biseo.kr"  # API domain for cloud URIs
# Only call the embedding API if colpali_mode is "api"
morphik_embedding_api_domain = "http://localhost:6000"  # endpoint for multivector embedding service
colpali_mode = "local" # "off", "local", or "api"

[redis]
host = "redis"  # use "redis" for docker
port = 6379

[graph]
model = "ollama_qwen_vision"
enable_entity_resolution = true

# [graph]
# mode="api"
# base_url="https://graph-api.morphik.ai"

[telemetry]
enabled = true
honeycomb_enabled = true
honeycomb_endpoint = "https://api.honeycomb.io"
honeycomb_proxy_endpoint = "https://otel-proxy.onrender.com"
service_name = "databridge-core"
otlp_timeout = 10
otlp_max_retries = 3
otlp_retry_delay = 1
otlp_max_export_batch_size = 512
otlp_schedule_delay_millis = 5000
otlp_max_queue_size = 2048
