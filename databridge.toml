[api]
host = "localhost"
port = 8000
reload = true

[auth]
jwt_algorithm = "HS256"
dev_mode = true  # Enabled by default for easier local development
dev_entity_id = "dev_user"  # Default dev user ID
dev_entity_type = "developer"  # Default dev entity type
dev_permissions = ["read", "write", "admin"]  # Default dev permissions

[completion]
provider = "ollama"
model_name = "llama3.2"
default_max_tokens = "1000"
default_temperature = 0.7
base_url = "http://ollama:11434"  # Just use the service name
# base_url = "http://localhost:11434" # Local configuration

# [completion]
# provider = "openai"
# model_name = "gpt-4-turbo-preview"
# default_max_tokens = "1000"
# default_temperature = 0.7

[database]
provider = "postgres"

# [database]
# provider = "mongodb"
# database_name = "databridge"
# collection_name = "documents"

[embedding]
provider = "ollama"
model_name = "nomic-embed-text"
dimensions = 768
similarity_metric = "cosine"
base_url = "http://ollama:11434"  # Just use the service name
# base_url = "http://localhost:11434" # Local configuration

# [embedding]
# provider = "openai"
# model_name = "text-embedding-3-small"
# dimensions = 1536
# similarity_metric = "dotProduct"


[parser]
provider = "unstructured"
chunk_size = 1000
chunk_overlap = 200
use_unstructured_api = false

# [parser]
# provider = "combined" | "contextual"
# chunk_size = 1000
# chunk_overlap = 200
# use_unstructured_api = false

[parser.vision]
provider = "ollama"
model_name = "llama3.2-vision"
frame_sample_rate = -1  # Set to -1 to disable frame captioning
# base_url = "http://localhost:11434"  # Only used for ollama
base_url = "http://ollama:11434"  # Use if using via docker

# [parser.vision]
# provider = "openai"
# model_name = "gpt-4o-mini"
# frame_sample_rate = -1 # Set to -1 to disable frame captioning

[reranker]
use_reranker = false
provider = "flag"
model_name = "BAAI/bge-reranker-large"
query_max_length = 256
passage_max_length = 512
use_fp16 = true
device = "mps"

# [reranker]
# use_reranker = false

[storage]
provider = "local"
storage_path = "./storage"

# [storage]
# provider = "aws-s3"
# region = "us-east-2"
# bucket_name = "databridge-s3-storage"

[vector_store]
provider = "pgvector"

# [vector_store]
# provider = "mongodb"
# database_name = "databridge"
# collection_name = "document_chunks"

[rules]
provider = "ollama"
model_name = "llama3.2"
batch_size = 4096
